{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpotifyHitPredictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSqFu5rmdTuk"
      },
      "source": [
        "Imports & Declaraciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayjSVs8EdS8s"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import requests\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWQjkXeUGo12"
      },
      "source": [
        "Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPoD9lLkTjs1"
      },
      "source": [
        "# Primera parte donde se unen los csv\n",
        "os.chdir(\"../Charts\")\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "\n",
        "df = pd.concat([pd.read_csv(f).drop(0) for f in all_filenames ],ignore_index=True)\n",
        "headers =  [\"Position\",\"Track Name\",\"Artist\",\"Streams\",\"URL\"]\n",
        "df.columns = headers\n",
        "\n",
        "# Creamos una columna de si una cancion es hit o no (1- Hit | 0- No Hit)\n",
        "df['es_hit'] = 0\n",
        "\n",
        "# Agregar hit a las primeras 50 filas (canciones de cada semana)\n",
        "for i in range(0, len(df),200):\n",
        "    for j in range(i,i+50):\n",
        "        df.at[j, 'es_hit'] = 1\n",
        "\n",
        "# Eliminamos los duplicados: En caso de que una canción repetida sea hit en un caso y en otro no, se considera hit\n",
        "df = df.sort_values(by='Position', ascending=True)\n",
        "df = df.drop_duplicates(subset='URL', keep=\"first\")\n",
        "\n",
        "#Desordenamos las filas y reseteamos los indices\n",
        "df = df.sample(frac=1.0, random_state=1).reset_index(drop=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaL03RRdsrI"
      },
      "source": [
        "Agregamos features de la API de Spotify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26T3_ABvds6N"
      },
      "source": [
        "# Me quedo con el id de la cancion\n",
        "df['URL'] = df['URL'].str[31:]\n",
        "\n",
        "# SPOTIFY\n",
        "\n",
        "CLIENT_ID  = \"*****************************\"\n",
        "CLIENT_SECRET  = \"****************************\"\n",
        "\n",
        "AUTH_URL = 'https://accounts.spotify.com/api/token'\n",
        "\n",
        "# POST\n",
        "auth_response = requests.post(AUTH_URL, {\n",
        "    'grant_type': 'client_credentials',\n",
        "    'client_id': CLIENT_ID,\n",
        "    'client_secret': CLIENT_SECRET,\n",
        "})\n",
        "\n",
        "# convert the response to JSON\n",
        "auth_response_data = auth_response.json()\n",
        "\n",
        "# save the access token\n",
        "access_token = auth_response_data['access_token']\n",
        "\n",
        "headers = {\n",
        "    'Authorization': 'Bearer {token}'.format(token=access_token)\n",
        "}\n",
        "\n",
        "# base URL of all Spotify API endpoints\n",
        "BASE_URL = 'https://api.spotify.com/v1/'\n",
        "\n",
        "df_features = pd.DataFrame()\n",
        "\n",
        "# Reinicio indices para hacer que coincidan\n",
        "df.reset_index(drop=True, inplace=True) \n",
        "\n",
        "# GET request a partir del ID de la canción\n",
        "for index, row in df.iterrows():\n",
        "    r = requests.get(BASE_URL + 'audio-features/' + row['URL'], headers=headers)\n",
        "    r = r.json()\n",
        "    #  Creamos un Dataframe con el json\n",
        "    df_json = pd.json_normalize(r)\n",
        "    # Añadimos la fila al Dataframe de features de Spotify\n",
        "    df_features = df_features.append(df_json, ignore_index=True)\n",
        "\n",
        "# Agregamos los nuevos features al dataframe original\n",
        "df = pd.concat([df, df_features], axis=1)\n",
        "\n",
        "# Guardamos el Dataframe en csv\n",
        "df.to_csv('datos_spotify.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq586FaKA96U"
      },
      "source": [
        "Leemos el CSV (no usamos el generado anteriormente, ya que cuando nos traemos los datos de la API desde COLAB nos genera muchos valores NaN, a diferencia de cuando lo hacemos en PyCharm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTSm5hT7fkuu"
      },
      "source": [
        "df = pd.read_csv('./datos_spotify.csv')\n",
        "df = df.sample(frac=1.0, random_state=1).reset_index(drop=True) #Desordena filas y reseteamos los indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY2piznSe3pe"
      },
      "source": [
        "Modelos de training y medidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umxqB-iTe5jE"
      },
      "source": [
        "layers = (6,6)  # Entrada + capa oculta\n",
        "modelos = {\n",
        "    \"                            Regresión logística\": LogisticRegression(random_state=1),\n",
        "    \"                            K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=4),\n",
        "    \"                              Árbol de decisión\": DecisionTreeClassifier(random_state=1),\n",
        "    \"Máquinas de vectores de soporte (Linear Kernel)\": LinearSVC(C=1.2,random_state=1),\n",
        "    \"   Máquinas de vectores de soporte (RBF Kernel)\": SVC(C=1.7,random_state=1),\n",
        "    \"                                   Red Neuronal\": MLPClassifier(solver='sgd',\n",
        "                        activation='logistic',\n",
        "                        alpha=1e-3,\n",
        "                        hidden_layer_sizes=layers,  # Neuronas en la capa de entrada y cada capa oculta\n",
        "                        random_state=1,\n",
        "                        max_iter=2000),\n",
        "    \"                               Bosque aleatorio\": RandomForestClassifier(n_estimators=500,random_state=1)\n",
        "}\n",
        "\n",
        "# Medidas\n",
        "medidas = []\n",
        "for i,_ in enumerate(modelos):\n",
        "  medidas.append({ 'accuracy': [], 'recall': [], 'f1': [] })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2MHDpfZeCsL"
      },
      "source": [
        "Manipulacion de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h15aj-E7eBh1"
      },
      "source": [
        "# Eliminamos los features descriptivos -> No aportan nada al dataset\n",
        "df = df.drop(['Position', 'Streams', 'Track Name', 'URL', 'id','track_href','analysis_url','type','uri'], axis=1)\n",
        "\n",
        "# Número de FOLDS\n",
        "Nfolds = 5\n",
        "\n",
        "y = df['es_hit']\n",
        "X = df.drop('es_hit', axis=1)\n",
        "\n",
        "# Codificación One Hot para los artistas\n",
        "X = pd.concat([X, pd.get_dummies(X['Artist'])], axis=1)\n",
        "X = X.drop(['Artist'],axis=1)\n",
        "\n",
        "y = y.to_numpy();\n",
        "X = X.to_numpy();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUuqS0UrBy1_"
      },
      "source": [
        "Validación cruzada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-K5ExVhBtDn"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=Nfolds, random_state=1)  # KFold estratificado\n",
        "\n",
        "for i,(train_index, test_index) in enumerate(skf.split(X,y)):\n",
        "    \n",
        "    print('\\n\\nProcesando partición {}/{}'.format(i+1,Nfolds))\n",
        "\n",
        "    #-----------------------------------\n",
        "    # Asignación de datos y etiquetas\n",
        "    # segun los índices del fold\n",
        "    #-----------------------------------\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    #------------------------------------------------\n",
        "    # Estandarización de los datos\n",
        "    # (sobre cada fold)\n",
        "    #------------------------------------------------\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    #------------------------------------------\n",
        "    # Ajuste del scaler con los datos de train\n",
        "    #------------------------------------------\n",
        "    scaler.fit(X_train)\n",
        "    # Transformación de datos train y test\n",
        "    #------------------------------------------\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    #Entrenamiento\n",
        "    for nombre_modelo, modelo in modelos.items():\n",
        "      modelo.fit(X_train, y_train)\n",
        "      print(nombre_modelo + \" entrenado.\")\n",
        "\n",
        "    #Testeo\n",
        "    j=0\n",
        "    for nombre_modelo, modelo in modelos.items():\n",
        "      y_pred = modelo.predict(X_test)\n",
        "\n",
        "      # Medidas de desempeño\n",
        "      _, recall, f1,_ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "      # Almacenamiento de las medidas\n",
        "      medidas[j]['accuracy'].append(accuracy)\n",
        "      medidas[j]['recall'].append(recall)\n",
        "      medidas[j]['f1'].append(f1)\n",
        "      j = j+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSCJgMJOM95G"
      },
      "source": [
        "Resultados de la validación cruzada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myr1LZGyNAyO"
      },
      "source": [
        "print('---                       k-Ffold CV                     ---')\n",
        "for i,nombre_modelo in enumerate(modelos):\n",
        "  print('---     Modelo:' + nombre_modelo + '     ---')\n",
        "  print('Fold  Acc   Recall   F1')\n",
        "  for fold in range(0,Nfolds):\n",
        "    print('{:3d} '.format(fold), \\\n",
        "          '{:.4f}'.format(medidas[i]['accuracy'][fold]),\\\n",
        "          '{:.4f}'.format(medidas[i]['recall'][fold]),\\\n",
        "          '{:.4f}'.format(medidas[i]['f1'][fold]))\n",
        "    \n",
        "  # Promedio de las medidas\n",
        "  print('Prom', \\\n",
        "      '{:.4f}'.format(np.mean(medidas[i]['accuracy'])), \\\n",
        "      '{:.4f}'.format(np.mean(medidas[i]['recall'])), \\\n",
        "      '{:.4f}'.format(np.mean(medidas[i]['f1'])))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}